{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lmar3213/miniconda3/envs/lew_conda/lib/python3.7/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn('LightFM was compiled without OpenMP support. '\n",
      "/Users/lmar3213/miniconda3/envs/lew_conda/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#normal tools:\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import utils\n",
    "\n",
    "#learning library:\n",
    "import lightfm\n",
    "\n",
    "#skopt:\n",
    "from skopt.space import Real, Integer\n",
    "from skopt import Optimizer\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from skopt.utils import use_named_args\n",
    "import skopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook compares wide- to long-format input data\n",
    "The interaction matrix is shape (n_users, n_items). If you have many users and few(er) items, there is a large difference in aspect ratio. Because of this, the WARP algorithm will behave differently. \n",
    "\n",
    "For example, with `n_users` down the vertical length, each protein gets updated `n_users` times per epoch. Conversely, each user will be updated `epoch` times in total. If you were to tranpose the matrix, each protein would recieve `n_items` updates per epoch - much fewer updates!\n",
    "\n",
    "The below code just demonstrates how, even if you're looking to optimize the ranking of ligands to be recommended to proteins, it's better to run the algorithm so that it optimizes the rankings of proteins to be recommended to ligands. The reason is that there are 100k's of ligands, so the proteins get 100k's of little updates each epoch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, run HPO\n",
    "This determines the 'best' hyperparameters for running LightFM with 'wide-format' data (i.e. shape (`n_items`, `n_users`), which is wider than it is long. \n",
    "\n",
    "Later this can be compared to the typical 'long-format' input, parameters for which are already available in file `hpo_lightfm_warp.dat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the 243-protein subset:\n",
    "interaction_matrix = utils.load_subset()\n",
    "\n",
    "##lightfm requires a particular way of getting the predictions:\n",
    "#lightfm 'user id' (chemical id)\n",
    "cid = np.arange(interaction_matrix.T.shape[0])\n",
    "#lightfm 'item id' (target id)\n",
    "tid = np.arange(interaction_matrix.T.shape[1])\n",
    "\n",
    "\n",
    "#this performs multiple repeats of the test/train split, if desired:\n",
    "def bootstrap(params, matrix, repeats):\n",
    "    results = list()\n",
    "    for _ in range(repeats):\n",
    "        #load a dataset:\n",
    "        train, test = utils.train_test_split(interaction_matrix, 0.05)\n",
    "        train = train.T\n",
    "        test = test.T\n",
    "        test = np.array(test.todense(), dtype=bool)\n",
    "        \n",
    "        #fit the model:\n",
    "        model = lightfm.LightFM(no_components = params['no_components'],\n",
    "                           loss='warp',\n",
    "                           max_sampled=params['max_sampled'],\n",
    "                           learning_rate=params['learning_rate'])\n",
    "        model.fit(train, epochs=params['epochs'])\n",
    "        \n",
    "        #make interaction predictions:\n",
    "        pred_matrix = model.predict(np.repeat(cid, len(tid)), np.tile(tid, len(cid)))\n",
    "        pred_matrix = np.reshape(pred_matrix, (len(cid), len(tid)))\n",
    "        \n",
    "        #evaluate by calculating mean rank:\n",
    "        #order from highest to lowest:\n",
    "        order = (-pred_matrix).argsort(axis=1)\n",
    "        #get ranks of each ligand.\n",
    "        ranks = order.argsort(axis=1)\n",
    "        mean_rank = np.mean(ranks[test])\n",
    "        results.append(mean_rank)\n",
    "        #results.append(-utils.evaluate_predictions(pred_matrix, train, test))\n",
    "    return np.mean(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "####SKOPT:\n",
    "\n",
    "#these are the hyperparameters and search spaces:\n",
    "space = [Integer(1, 400, name='no_components'),\n",
    "        Integer(1,15, name='max_sampled'),\n",
    "        Real(10**-5, 10**0, \"log-uniform\", name='learning_rate'),\n",
    "        Integer(1,20, name='epochs')]\n",
    "\n",
    "#the objective function for skopt:\n",
    "@use_named_args(space)\n",
    "def score(**params):\n",
    "    score = bootstrap(params, interaction_matrix, 1)\n",
    "    return (score)\n",
    "\n",
    "optimizer = Optimizer(dimensions=space,\n",
    "                     random_state=1,\n",
    "                     base_estimator='ET',\n",
    "                     n_random_starts=12)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    x = optimizer.ask(n_points=4)\n",
    "    y = Parallel(n_jobs=4)(delayed(score)(v) for v in x)\n",
    "    optimizer.tell(x,y)\n",
    "\n",
    "result = skopt.utils.create_result(optimizer.Xi,\n",
    "                                  optimizer.yi,\n",
    "                                  optimizer.space,\n",
    "                                  optimizer.rng,\n",
    "                                  models=optimizer.models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5457.995290423862"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 5, 0.013877212573012852, 17]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##These are the best hyperparameters for wide-format data:\n",
    "\n",
    "result.x_iters[np.argmin(result.func_vals)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, compare wide- and long- format using a single hyperparameter set.\n",
    "\n",
    "This uses the best parameters from `hpo_lightfm_warp.dat`\n",
    "\n",
    "It shows that mean rank (determined wide-ways) is better using the long-format input. \n",
    "\n",
    "'wide-ways' means the mean rank is a lot larger! That's simply due to there being a lot of ligands. It means that the ranks go all the way up ~100k's. Whereas calculating the long-ways ranks only goes up to about 250 (the number of proteins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long format mean rank:\n",
      "4155.326234867898\n"
     ]
    }
   ],
   "source": [
    "train, test = utils.train_test_split(interaction_matrix, 0.05)\n",
    "test = np.array(test.todense(), dtype=bool)\n",
    "\n",
    "cid = np.arange(train.shape[0])\n",
    "tid = np.arange(train.shape[1])\n",
    "model = lightfm.LightFM(no_components = 127,\n",
    "                           loss='warp',\n",
    "                           max_sampled=9,\n",
    "                           learning_rate=0.0561)\n",
    "model.fit(train, epochs=6)\n",
    "\n",
    "pred_matrix = model.predict(np.repeat(cid, len(tid)), np.tile(tid, len(cid)))\n",
    "pred_matrix = np.reshape(pred_matrix, (len(cid), len(tid)))\n",
    "\n",
    "#trained in wide format, now convert to long format to compare with long-format training:\n",
    "pred_matrix = pred_matrix.T\n",
    "#order from highest to lowest:\n",
    "order = (-pred_matrix).argsort(axis=1)\n",
    "#get ranks of each ligand.\n",
    "ranks = order.argsort(axis=1)\n",
    "print('long format mean rank:')\n",
    "print(np.mean(ranks[test.T])) #use test.T to compare fairly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wide format mean rank:\n",
      "5585.520190863303\n"
     ]
    }
   ],
   "source": [
    "train = train.T\n",
    "\n",
    "cid = np.arange(train.shape[0])\n",
    "tid = np.arange(train.shape[1])\n",
    "model = lightfm.LightFM(no_components = 127,\n",
    "                           loss='warp',\n",
    "                           max_sampled=9,\n",
    "                           learning_rate=0.0561)\n",
    "model.fit(train, epochs=6)\n",
    "\n",
    "pred_matrix = model.predict(np.repeat(cid, len(tid)), np.tile(tid, len(cid)))\n",
    "pred_matrix = np.reshape(pred_matrix, (len(cid), len(tid)))\n",
    "\n",
    "#order from highest to lowest:\n",
    "order = (-pred_matrix).argsort(axis=1)\n",
    "#get ranks of each ligand.\n",
    "ranks = order.argsort(axis=1)\n",
    "print('wide format mean rank:')\n",
    "print(np.mean(ranks[test.T])) #use test.T to compare fairly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, compare wide- and long- format with their respective best hyperparameters\n",
    "\n",
    "Long input format is _still_ better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wide format mean rank:\n",
      "5441.103030838562\n"
     ]
    }
   ],
   "source": [
    "model = lightfm.LightFM(no_components = 20,\n",
    "                           loss='warp',\n",
    "                           max_sampled=5,\n",
    "                           learning_rate=0.01387)\n",
    "model.fit(train, epochs=17)\n",
    "\n",
    "\n",
    "\n",
    "pred_matrix = model.predict(np.repeat(cid, len(tid)), np.tile(tid, len(cid)))\n",
    "pred_matrix = np.reshape(pred_matrix, (len(cid), len(tid)))\n",
    "\n",
    "#order from highest to lowest:\n",
    "order = (-pred_matrix).argsort(axis=1)\n",
    "#get ranks of each ligand.\n",
    "ranks = order.argsort(axis=1)\n",
    "print('wide format mean rank:')\n",
    "print(np.mean(ranks[test.T])) #use test.T to compare fairly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
